{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e049ce06-3976-45c3-a838-9a453a3a91a3",
   "metadata": {},
   "source": [
    "## 다양한 LLM활용방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5897c1f7-d2f3-42f6-8ea2-5da44d822eb1",
   "metadata": {},
   "source": [
    "#### OpenAI(gpt-4o) - 1 OpenAI API 직접 활용, 2 LangChain활용\n",
    "#### Anthropic(claude3.5-haiku) - 2 Anthropic API 직접활용,  2 LangChain활용\n",
    "#### Ollama(오픈소스+로컬) - 1 Ollama API직접활용,  2 LangChain활용\n",
    "#### Groq(오픈소스+로컬) - 1 Groq API 직접활용,  2 LangChain활용\n",
    "#### 즉, 각각의 API 활용하거나, wrapping된 LangChain라이브러리 사용하거나 2가지 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36abef3-bfa7-43c1-a988-5fbef197c626",
   "metadata": {},
   "source": [
    "### 1-1. OpenAI 라이브러리 활용 API호출\n",
    "\n",
    "#### https://platform.openai.com/docs/guides/streaming-responses?api-mode=chat 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "705f97b7-7fd7-4b22-b63f-2387c27d5ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last knowledge update in October 2023, Joe Biden is the President of the United States. He took office on January 20, 2021. Please verify with a current source to ensure this information is still accurate."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "              {\"role\": \"user\", \"content\": \"who is the president of the US?\"}\n",
    "             ],\n",
    "    stream=True,\n",
    " )\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4da8dfe-eaec-471a-bc18-c6cc1daf6c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BZqR8mL5CTE91SdUGeEOUzL79Ir1o', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='As of my last update in October 2023, the President of the United States is Joe Biden. He took office on January 20, 2021. Please verify with a current source to ensure this information is still accurate, as my data may not reflect recent changes.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))], created=1747882746, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_54eb4bd693', usage=CompletionUsage(completion_tokens=56, prompt_tokens=24, total_tokens=80, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "              {\"role\": \"user\", \"content\": \"who is the president of the US?\"}\n",
    "             ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd2dbe52-efec-470d-80a4-3ef848cf08e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-BZqR8mL5CTE91SdUGeEOUzL79Ir1o',\n",
       " 'choices': [{'finish_reason': 'stop',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'message': {'content': 'As of my last update in October 2023, the President of the United States is Joe Biden. He took office on January 20, 2021. Please verify with a current source to ensure this information is still accurate, as my data may not reflect recent changes.',\n",
       "    'refusal': None,\n",
       "    'role': 'assistant',\n",
       "    'audio': None,\n",
       "    'function_call': None,\n",
       "    'tool_calls': None,\n",
       "    'annotations': []}}],\n",
       " 'created': 1747882746,\n",
       " 'model': 'gpt-4o-mini-2024-07-18',\n",
       " 'object': 'chat.completion',\n",
       " 'service_tier': 'default',\n",
       " 'system_fingerprint': 'fp_54eb4bd693',\n",
       " 'usage': {'completion_tokens': 56,\n",
       "  'prompt_tokens': 24,\n",
       "  'total_tokens': 80,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e09f2501-7ef5-4b3f-beed-420dea36ebda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of my last update in October 2023, the President of the United States is Joe Biden. He took office on January 20, 2021. Please verify with a current source to ensure this information is still accurate, as my data may not reflect recent changes.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d223e9e-4523-4a74-aa5e-1ab19fbdc51b",
   "metadata": {},
   "source": [
    "### 1-2. Langchain의 OpenAI API호출\n",
    "#### Lnagchain에 wrapping된 OpenAI 라이브러리 활용 API호출\n",
    "#### AIMessage는 Langchain에서 사용하는 표현 assistant message라고 보면 된다. 단일 메세지 다중 메세지 모두 처리 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74d92c35-da02-4adc-91e0-517b93ce96d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As of my last update in October 2023, the President of the United States is Joe Biden. He took office on January 20, 2021. Please verify with up-to-date sources to confirm if he is still the current president.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 15, 'total_tokens': 65, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_54eb4bd693', 'finish_reason': 'stop', 'logprobs': None}, id='run-269897e1-2026-406b-853e-f04cc76ce76e-0', usage_metadata={'input_tokens': 15, 'output_tokens': 50, 'total_tokens': 65, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "lo_resp = model.invoke(\"who is the president of the US?\")\n",
    "lo_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9ed2757-7e26-457d-bf3f-36e0020777d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of my last update in October 2023, the President of the United States is Joe Biden. He took office on January 20, 2021. Please verify with up-to-date sources to confirm if he is still the current president.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lo_resp.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0169e5c8-deb8-4426-a726-ddbad1b98604",
   "metadata": {},
   "source": [
    "#### AIMessage는 langchain에서 사용되는 객체\n",
    "##### 다중메세지도 처리 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e7597dd-506d-426a-bb94-2a2a5cc4f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "              {\"role\": \"user\", \"content\": \"who is the president of the US?\"}\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8d4bbde-2b0c-47a3-8dc6-1fd63568fade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As of my last knowledge update in October 2023, the President of the United States is Joe Biden. He took office on January 20, 2021. Please verify with up-to-date sources, as my information may not reflect the latest developments.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 24, 'total_tokens': 76, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_54eb4bd693', 'finish_reason': 'stop', 'logprobs': None}, id='run-3d88e561-ccb5-45ad-9990-d0036cf53b89-0', usage_metadata={'input_tokens': 24, 'output_tokens': 52, 'total_tokens': 76, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e6c51-fa3c-44c4-934c-d1784b68cc89",
   "metadata": {},
   "source": [
    "#### openai와 dict의 구조가 약간 다르다. openai는 choice로 시작 langchain은 content부터 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afff6882-0e3c-47c0-aa53-042d0bf7b8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'As of my last knowledge update in October 2023, the President of the United States is Joe Biden. He took office on January 20, 2021. Please verify with up-to-date sources, as my information may not reflect the latest developments.',\n",
       " 'additional_kwargs': {'refusal': None},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 52,\n",
       "   'prompt_tokens': 24,\n",
       "   'total_tokens': 76,\n",
       "   'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "    'audio_tokens': 0,\n",
       "    'reasoning_tokens': 0,\n",
       "    'rejected_prediction_tokens': 0},\n",
       "   'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "  'model_name': 'gpt-4o-mini-2024-07-18',\n",
       "  'system_fingerprint': 'fp_54eb4bd693',\n",
       "  'finish_reason': 'stop',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run-3d88e561-ccb5-45ad-9990-d0036cf53b89-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 24,\n",
       "  'output_tokens': 52,\n",
       "  'total_tokens': 76,\n",
       "  'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "  'output_token_details': {'audio': 0, 'reasoning': 0}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18f5d67-4a26-41b3-93a9-e9bbf7bcd7d9",
   "metadata": {},
   "source": [
    "#### message의 내용출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cfbe210-021a-4184-b083-8a402f989b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of my last knowledge update in October 2023, the President of the United States is Joe Biden. He took office on January 20, 2021. Please verify with up-to-date sources, as my information may not reflect the latest developments.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b887d-be3d-479a-b221-e5dab29e8d07",
   "metadata": {},
   "source": [
    "#### 토큰의 사용량등을 표현하는 meta데이터도 출력가능하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21c5d1a8-d269-43ce-b103-5804174b1168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 24,\n",
       " 'output_tokens': 52,\n",
       " 'total_tokens': 76,\n",
       " 'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       " 'output_token_details': {'audio': 0, 'reasoning': 0}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aceea4-d063-44c9-8cc5-fa93dee0d592",
   "metadata": {},
   "source": [
    "### 2-1. Anthropic 라이브러리 API호출\n",
    "#### Anthropic 라이브러리 활용 API호출\n",
    "#### https://docs.anthropic.com/en/docs/initial-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44493b4a-21e2-438b-9e15-ccb925c140d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(citations=None, text=\"# Tears of Time\\n\\nEarth's ancient tears,\\nMinerals dissolved through years,\\nRivers carry tales to sea,\\nSalt remains eternally.\", type='text')]\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-7-sonnet-20250219\",\n",
    "    max_tokens=1000,\n",
    "    temperature=1,\n",
    "    system=\"You are a world-class poet. Respond only with short poems.\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Why is the ocean salty?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5894cb3b-0723-44c3-8c9f-a9ed4089488a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'msg_01PkzW19M53x8KHXGotm86hV',\n",
       " 'content': [{'citations': None,\n",
       "   'text': \"# Tears of Time\\n\\nEarth's ancient tears,\\nMinerals dissolved through years,\\nRivers carry tales to sea,\\nSalt remains eternally.\",\n",
       "   'type': 'text'}],\n",
       " 'model': 'claude-3-7-sonnet-20250219',\n",
       " 'role': 'assistant',\n",
       " 'stop_reason': 'end_turn',\n",
       " 'stop_sequence': None,\n",
       " 'type': 'message',\n",
       " 'usage': {'cache_creation_input_tokens': 0,\n",
       "  'cache_read_input_tokens': 0,\n",
       "  'input_tokens': 29,\n",
       "  'output_tokens': 34}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27a53a06-0436-4b8c-9a45-2cf53dc9fe23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Tears of Time\\n\\nEarth's ancient tears,\\nMinerals dissolved through years,\\nRivers carry tales to sea,\\nSalt remains eternally.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e9d2fa0-3e1d-4b8f-8183-5c8913bf4ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tears of Time\n",
      "\n",
      "Earth's ancient tears,\n",
      "Minerals dissolved through years,\n",
      "Rivers carry tales to sea,\n",
      "Salt remains eternally.\n"
     ]
    }
   ],
   "source": [
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1167cd-ff2f-4367-b188-dae1e53a8731",
   "metadata": {},
   "source": [
    "### 2-2. Langchain의 Anthropic 라이브러리 API호출 v0.2 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44414e9c-06b2-4353-af4b-5a88c509e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-7-sonnet-20250219\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c172c1c-0d96-4205-82e2-dbf771b564d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = model.invoke('Why is the ocean salty?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d4e0f6d-584f-4e14-9be2-431a79595ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'The ocean is salty primarily because of dissolved minerals that have been carried into it by rivers over billions of years. When rainwater flows over land, it weathers rocks and soil, picking up minerals like sodium and chloride (which form salt). These minerals accumulate in the oceans while the water cycles back into the atmosphere through evaporation, leaving the salt behind. Underwater volcanic activity and hydrothermal vents also contribute minerals to the ocean. Over time, this continuous process has resulted in our oceans containing about 35 grams of salt per liter of seawater on average.',\n",
       " 'additional_kwargs': {},\n",
       " 'response_metadata': {'id': 'msg_0155GXr7WZMjxxkKLPjk5YuL',\n",
       "  'model': 'claude-3-7-sonnet-20250219',\n",
       "  'stop_reason': 'end_turn',\n",
       "  'stop_sequence': None,\n",
       "  'usage': {'cache_creation_input_tokens': 0,\n",
       "   'cache_read_input_tokens': 0,\n",
       "   'input_tokens': 14,\n",
       "   'output_tokens': 128}},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run-b0097739-d7b8-4ff7-bdc7-b1f31ed96d60-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 14,\n",
       "  'output_tokens': 128,\n",
       "  'total_tokens': 142,\n",
       "  'input_token_details': {'cache_read': 0, 'cache_creation': 0}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d812ff8-466c-4afa-bcad-a761fefb5603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The ocean is salty primarily because of dissolved minerals that have been carried into it by rivers over billions of years. When rainwater flows over land, it weathers rocks and soil, picking up minerals like sodium and chloride (which form salt). These minerals accumulate in the oceans while the water cycles back into the atmosphere through evaporation, leaving the salt behind. Underwater volcanic activity and hydrothermal vents also contribute minerals to the ocean. Over time, this continuous process has resulted in our oceans containing about 35 grams of salt per liter of seawater on average.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bfbd35-6aba-4569-99bd-8fb324686d53",
   "metadata": {},
   "source": [
    "### 2-2. Langchain의 Anthropic 라이브러리 API호출 v0.3 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0a28eb0-2e4f-4caa-a8e7-56b44f5c71ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"claude-3-7-sonnet-20250219\", model_provider=\"anthropic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb94c2f3-13eb-454a-8d52-120df36690e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello there! It's great to connect with you. How can I help you today? I'm ready to answer questions, have a conversation, or assist with information on a wide variety of topics. What would you like to talk about?\", additional_kwargs={}, response_metadata={'id': 'msg_018K1hYfNMN1QmcEr6yc3Uh9', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 11, 'output_tokens': 51}}, id='run-6d1861d9-5302-4d9e-835b-42e06f52ef08-0', usage_metadata={'input_tokens': 11, 'output_tokens': 51, 'total_tokens': 62, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384aa740-3b58-404f-be3a-5447cdfe3cb1",
   "metadata": {},
   "source": [
    "#### OpenAI message형식을 그대로 사용하도록 wrapping되어 있다.\n",
    "#### 즉, anthropic 자체에서는 system 메세지를 별도로 분리해두었지만 langchain에서는 통합하면서 그대로 사용하도록 되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b4581fa-ea13-4878-a53c-5b13876d4450",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "              {\"role\": \"user\", \"content\": \"who is the president of the US?\"}\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a693959-c5c2-40a3-83ed-55add5b2104b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The current President of the United States is Joe Biden. He was inaugurated as the 46th President on January 20, 2021, following his victory in the 2020 presidential election.', additional_kwargs={}, response_metadata={'id': 'msg_01NxSro3B3RwysGvPdEjRxW2', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 20, 'output_tokens': 45}}, id='run-4e66ca57-64fd-4f40-ae8b-48fcb6a2e222-0', usage_metadata={'input_tokens': 20, 'output_tokens': 45, 'total_tokens': 65, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8000fb6-d093-4f0d-a012-e273dd6f9772",
   "metadata": {},
   "source": [
    "### 3-1. Ollama를 이용한 로컬모델 사용방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51385e66-0f3f-4e1a-a124-5849d58884d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               ID              SIZE      MODIFIED     \n",
      "mistral:latest     f974a74358d6    4.1 GB    3 months ago    \n",
      "llama3.2:latest    a80c4f17acd5    2.0 GB    3 months ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05ef0b-7514-4f0c-bf21-83f9694bf45f",
   "metadata": {},
   "source": [
    "#### ollama내 모델을 사용할때는 endpoint url을 통해 api로 호출하여 사용할수 있다. 11434포트를 기본포트를 사용한다. generate모드과 chat모드의 차이는 입력문장이 하나, 메세지 여러개 나누어서 입력가능한 형태 차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "471c1b90-3641-4e28-bf3c-2f7588d836d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatAnthropic(model='claude-3-7-sonnet-20250219', anthropic_api_url='https://api.anthropic.com', anthropic_api_key=SecretStr('**********'), model_kwargs={})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc260f57-50ec-46dc-9050-46b59e32a810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3.2',\n",
       " 'created_at': '2025-05-22T03:12:28.721126Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': 'Large Language Model (LLM)은深層 신경망을 사용하여 natural language processing(보조 언어 처리)과 text generation에 대한 성능이 높은 기계학習 알고리즘입니다. LLM의 동기는 특정 텍스트 데이터에 학습된 후, 해당 데이터를 기반으로 다른-text 또는 new-text의 생성을 가능하게 만듭니다.\\n\\nLLM은 크게 세 가지 주요 부분으로 구성됩니다:\\n\\n1.  **데이터セット**: LLM은 텍스트 데이터セット을 사용하여 학습합니다. 이 데이터erset은 신경망이 학습할 수 있는 텍스트를 포함합니다.\\n2.  **인퍼런스**: 텍스트에 대한 인퍼런스는 LLM의 텍스트 generation 성능을 향상시키는 데 중요한 역할을 합니다. 이는 텍스트의 단어 순서, 문장 구조 및 의미를 예측하는 데 사용됩니다.\\n3.  **학습**: LLM은 데이터セット과 인퍼런스를 sử dụng하여 학습합니다. 이 학습 과정에서는 신경망이 데이터セット에 대한 지식을 acquisition하는 process가 있습니다.\\n\\nLLM의 동기는 텍스트 generation, natural language understanding 및 text-to-text task에 대해 매우 높은 성능을 제공합니다. LLM은 다음과 같은 특징을 가집니다:\\n\\n*   **Natural Language Understanding(NLU)**: LLM은 텍스트에서 의미를 이해하는 데 사용됩니다.\\n*   1  **Text Generation(TG)**: LLM은 텍스트를 생성하는 데 사용됩니다.\\n*   **Chatbot 및 conversational AI**: LLM은 chatbot 및 conversational AI에 대한 성능을 향상시키는 데 widely used합니다.\\n\\nLLM은 자연어ประมวล도( Natural Language Processing, NLP) 분야에서 매우 중요한 역할을 하고 있습니다.'},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 19734199708,\n",
       " 'load_duration': 6594721875,\n",
       " 'prompt_eval_count': 41,\n",
       " 'prompt_eval_duration': 3816980667,\n",
       " 'eval_count': 392,\n",
       " 'eval_duration': 9313150958}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def ollama_chat(model='llama3.2', messages=[]):\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/chat\",\n",
    "        json = {\"model\": model, \"messages\": messages, \"stream\": False},\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "response = ollama_chat(\n",
    "    model=\"llama3.2\",\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\":\"LLM은 어떤 원리로 작동하나요?\"},\n",
    "    ]\n",
    "    \n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e730ff4-b5cf-4566-aabd-292319d1be37",
   "metadata": {},
   "source": [
    "### 3-2. Langchain의 Ollama를 이용한 로컬모델 사용방법\n",
    "#### https://python.langchain.com/docs/integrations/chat/ollama/ 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68a0670c-dbd4-4b2c-9190-7e3a5cd4b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0,\n",
    "    num_predict = 1024, #max token\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8df34d6d-ab92-4e96-b1ab-1c2cf00c620e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LLM(Large Language Model)은 인공지능 기술 중 하나입니다. LLM은 대규모 언어 데이터를 기반으로 작동하며, 자연어 처리 task에 대한 성능이 매우 높습니다.\\n\\nLLM의 원리는 다음과 같습니다.\\n\\n1. **대규모 언어 데이터**: LLM은 대규모 언어 데이터를 기반으로 작동합니다. 이 데이터는 millions 또는 billions 개의 텍스트 데이터가 포함되어 있습니다.\\n2. **네트워크 구조**: LLM은 특정 네트워크 구조를 사용하여 데이터를 처리합니다. 일반적으로, LLM은 transformers와 같은 Self-Attention Mechanism을 사용합니다.\\n3. **Self-Attention Mechanism**: Self-Attention Mechanism은 각 단어의 의미를 다른 단어와 비교하여 파악하는 mechanism입니다. 이 mechanism은 각 단어가 다른 단어와 어떻게 관련이 있는지 파악하여 문장을 이해하는 데 도움이 됩니다.\\n4. **다양한 task에 대한 성능**: LLM은 다양한 자연어 처리 task에 대해 매우 높은 성능을 보여줍니다. 예를 들어, Natural Language Processing (NLP), Text Classification, Sentiment Analysis, Question Answering, etc.\\n\\nLLM의 작동 원리는 다음과 같습니다.\\n\\n1. **문장 입력**: 사용자가 LLM에게 문장을 입력합니다.\\n2. **문장 파싱**: LLM은 문장을 파싱하여 단어를 분리합니다.\\n3. **Self-Attention Mechanism**: LLM은 Self-Attention Mechanism을 사용하여 각 단어의 의미를 다른 단어와 비교합니다.\\n4. **문장 이해**: LLM은 문장을 이해하기 위해 Self-Attention Mechanism과 다른 mechanism을 사용합니다.\\n5. **output**: LLM은 output을 생성합니다.\\n\\nLLM은 매우 powerful한 자연어 처리 기술입니다. 그러나, LLM의 성능은 데이터의 질과 양에 따라 달라집니다.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-05-22T03:13:21.42184Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10085538417, 'load_duration': 31722958, 'prompt_eval_count': 49, 'prompt_eval_duration': 205918084, 'eval_count': 410, 'eval_duration': 9846568000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-681bea8e-3176-4610-8d89-82aaf567b2db-0', usage_metadata={'input_tokens': 49, 'output_tokens': 410, 'total_tokens': 459})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\":\"LLM은 어떤 원리로 작동하나요? 한글로 답변부탁드립니다.\"},\n",
    "    ]\n",
    "ai_msgs = llm.invoke(messages)\n",
    "ai_msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eed565ee-00f8-4162-9b2e-9bf5cd7e6afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLM(Large Language Model)은 인공지능 기술 중 하나입니다. LLM은 대규모 언어 데이터를 기반으로 작동하며, 자연어 처리 task에 대한 성능이 매우 높습니다.\\n\\nLLM의 원리는 다음과 같습니다.\\n\\n1. **대규모 언어 데이터**: LLM은 대규모 언어 데이터를 기반으로 작동합니다. 이 데이터는 millions 또는 billions 개의 텍스트 데이터가 포함되어 있습니다.\\n2. **네트워크 구조**: LLM은 특정 네트워크 구조를 사용하여 데이터를 처리합니다. 일반적으로, LLM은 transformers와 같은 Self-Attention Mechanism을 사용합니다.\\n3. **Self-Attention Mechanism**: Self-Attention Mechanism은 각 단어의 의미를 다른 단어와 비교하여 파악하는 mechanism입니다. 이 mechanism은 각 단어가 다른 단어와 어떻게 관련이 있는지 파악하여 문장을 이해하는 데 도움이 됩니다.\\n4. **다양한 task에 대한 성능**: LLM은 다양한 자연어 처리 task에 대해 매우 높은 성능을 보여줍니다. 예를 들어, Natural Language Processing (NLP), Text Classification, Sentiment Analysis, Question Answering, etc.\\n\\nLLM의 작동 원리는 다음과 같습니다.\\n\\n1. **문장 입력**: 사용자가 LLM에게 문장을 입력합니다.\\n2. **문장 파싱**: LLM은 문장을 파싱하여 단어를 분리합니다.\\n3. **Self-Attention Mechanism**: LLM은 Self-Attention Mechanism을 사용하여 각 단어의 의미를 다른 단어와 비교합니다.\\n4. **문장 이해**: LLM은 문장을 이해하기 위해 Self-Attention Mechanism과 다른 mechanism을 사용합니다.\\n5. **output**: LLM은 output을 생성합니다.\\n\\nLLM은 매우 powerful한 자연어 처리 기술입니다. 그러나, LLM의 성능은 데이터의 질과 양에 따라 달라집니다.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msgs.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08605ece-b56b-40ee-9a18-3c154531efab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 49, 'output_tokens': 410, 'total_tokens': 459}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msgs.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edc154b-d980-427c-a3c8-f5055e6fa5c6",
   "metadata": {},
   "source": [
    "### 4-1. groq api를 이용한 다양한 모델 사용방법\n",
    "#### groq은 추론전용서비스 npu groq자체 하드웨어를 가지고 있어 빠른게 장점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c048ad2-0793-4ec1-ba70-de85bdc22a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3.2',\n",
       " 'created_at': '2025-05-22T03:12:28.721126Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': 'Large Language Model (LLM)은深層 신경망을 사용하여 natural language processing(보조 언어 처리)과 text generation에 대한 성능이 높은 기계학習 알고리즘입니다. LLM의 동기는 특정 텍스트 데이터에 학습된 후, 해당 데이터를 기반으로 다른-text 또는 new-text의 생성을 가능하게 만듭니다.\\n\\nLLM은 크게 세 가지 주요 부분으로 구성됩니다:\\n\\n1.  **데이터セット**: LLM은 텍스트 데이터セット을 사용하여 학습합니다. 이 데이터erset은 신경망이 학습할 수 있는 텍스트를 포함합니다.\\n2.  **인퍼런스**: 텍스트에 대한 인퍼런스는 LLM의 텍스트 generation 성능을 향상시키는 데 중요한 역할을 합니다. 이는 텍스트의 단어 순서, 문장 구조 및 의미를 예측하는 데 사용됩니다.\\n3.  **학습**: LLM은 데이터セット과 인퍼런스를 sử dụng하여 학습합니다. 이 학습 과정에서는 신경망이 데이터セット에 대한 지식을 acquisition하는 process가 있습니다.\\n\\nLLM의 동기는 텍스트 generation, natural language understanding 및 text-to-text task에 대해 매우 높은 성능을 제공합니다. LLM은 다음과 같은 특징을 가집니다:\\n\\n*   **Natural Language Understanding(NLU)**: LLM은 텍스트에서 의미를 이해하는 데 사용됩니다.\\n*   1  **Text Generation(TG)**: LLM은 텍스트를 생성하는 데 사용됩니다.\\n*   **Chatbot 및 conversational AI**: LLM은 chatbot 및 conversational AI에 대한 성능을 향상시키는 데 widely used합니다.\\n\\nLLM은 자연어ประมวล도( Natural Language Processing, NLP) 분야에서 매우 중요한 역할을 하고 있습니다.'},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 19734199708,\n",
       " 'load_duration': 6594721875,\n",
       " 'prompt_eval_count': 41,\n",
       " 'prompt_eval_duration': 3816980667,\n",
       " 'eval_count': 392,\n",
       " 'eval_duration': 9313150958}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "              {\"role\": \"user\", \"content\": \"who is the president of the US?\"}\n",
    "             ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77666a-0f58-4f98-8f30-080dce125d2a",
   "metadata": {},
   "source": [
    "### 4-2. Langchain의 groq api를 이용한 다양한 모델 사용방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5b4f9ce-3aaf-43be-aab5-623d0c86a3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='나는 프로그래밍을 사랑해요. \\n\\n(naneun peurogeuraemingeul saranghaeyo.)\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 29, 'total_tokens': 61, 'completion_time': 0.058181818, 'prompt_time': 0.002188435, 'queue_time': 0.016512953, 'total_time': 0.060370253}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-3ebc4fb7-679e-4da7-9ac8-d2290097fc97-0', usage_metadata={'input_tokens': 29, 'output_tokens': 32, 'total_tokens': 61})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"gemma2-9b-it\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to Korean. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"user\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aac74f50-7837-47fd-a4af-10cee572a4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 프로그래밍을 사랑해요. \n",
      "\n",
      "(naneun peurogeuraemingeul saranghaeyo.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e51a0e-6501-40a2-a7e3-82ee9561f459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
